{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JUSeg7-BkA_v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers faiss-cpu PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TirSkDDwwP7H",
        "outputId": "f6dd9797-5ffa-481e-8bb3-5458bd4ef5b2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.11)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "import PyPDF2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import pipeline\n",
        "\n",
        "# Step 1: Extract Text from PDF File\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with open(pdf_file, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()  # Extract text from each page\n",
        "    return text\n",
        "\n",
        "# Step 2: Clean and Preprocess the Text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGd1CTGL8_q8",
        "outputId": "a9fdc004-d221-400c-8f8f-2ee2c8f66252"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split Text into Sentences\n",
        "def split_text(text):\n",
        "    return sent_tokenize(text)  # Split text into sentences using NLTK\n",
        "\n",
        "# Step 4: Create Embeddings for each sentence/paragraph\n",
        "def create_embeddings(text_chunks, model):\n",
        "    embeddings = model.encode(text_chunks, convert_to_numpy=True)  # Create embeddings for each text chunk\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "b8axhoXv9EQE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build FAISS Index with Cosine Similarity\n",
        "def build_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]  # Get the dimensionality of the embeddings\n",
        "    index = faiss.IndexFlatIP(dimension)  # Using Inner Product (Cosine Similarity)\n",
        "    faiss.normalize_L2(embeddings)  # Normalize embeddings for cosine similarity\n",
        "    index.add(embeddings)  # Add the embeddings to the index\n",
        "    return index"
      ],
      "metadata": {
        "id": "bPJc34j59JRZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Query and Retrieve Relevant Sentences\n",
        "def retrieve_similar_sentences(query, model, index, text_chunks, threshold=0.3):\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)  # Create an embedding for the query\n",
        "    faiss.normalize_L2(query_embedding)  # Normalize the query embedding for cosine similarity\n",
        "    distances, indices = index.search(query_embedding, 3)  # Search for the top 3 nearest embeddings\n",
        "    results = [text_chunks[i] for i, distance in zip(indices[0], distances[0]) if distance > threshold]\n",
        "    return results if results else [\"No relevant sentences found.\"]\n"
      ],
      "metadata": {
        "id": "zptsoM-a9Lxi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Curate Response Based on System Message\n",
        "def curate_response(similar_sentences, system_message):\n",
        "    if system_message == \"summarize\":\n",
        "        summarizer = pipeline(\"summarization\")  # Use a summarization model\n",
        "        summarized_text = summarizer(' '.join(similar_sentences), max_length=130, min_length=30, do_sample=False)\n",
        "        return summarized_text[0]['summary_text']\n",
        "\n",
        "    elif system_message == \"detailed response\":\n",
        "        return '\\n\\n'.join(similar_sentences)\n",
        "\n",
        "    elif system_message == \"insights only\":\n",
        "        insights = [sentence for sentence in similar_sentences if sentence.startswith('-')]  # Extract bullet points\n",
        "        return '\\n'.join(insights) if insights else \"No key insights found.\"\n",
        "\n",
        "    else:\n",
        "        return '\\n\\n'.join(similar_sentences)"
      ],
      "metadata": {
        "id": "NCKfDxk89N63"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Extract text from a PDF file\n",
        "    pdf_file = '/content/apple-products.pdf'\n",
        "    raw_text = extract_text_from_pdf(pdf_file)\n",
        "\n",
        "    # Step 2: Clean and preprocess the text\n",
        "    cleaned_text = clean_text(raw_text)\n",
        "\n",
        "    # Step 3: Split the text into sentences\n",
        "    text_chunks = split_text(cleaned_text)\n",
        "\n",
        "    # Step 4: Load the embedding model and create embeddings for each sentence\n",
        "    model = SentenceTransformer('paraphrase-mpnet-base-v2')  # A larger, more robust model for better embeddings\n",
        "    embeddings = create_embeddings(text_chunks, model)\n",
        "\n",
        "    # Step 5: Build the FAISS index\n",
        "    index = build_faiss_index(embeddings)\n",
        "\n",
        "    print(\"Welcome to the RAG Bot!\")\n",
        "    print(\"Ask your question, or type 'exit' to quit.\")\n",
        "    system_message = 'detailed response'\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nYou: \")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Step 6: Retrieve relevant sentences based on the query\n",
        "        similar_sentences = retrieve_similar_sentences(query, model, index, text_chunks)\n",
        "\n",
        "        # Step 7: Curate the response based on system message\n",
        "        curated_response = curate_response(similar_sentences, system_message)\n",
        "\n",
        "        # Display result\n",
        "        print(f\"Bot:\\n{curated_response}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltvh_Rgfwrml",
        "outputId": "a63604f6-7ed9-480a-b34b-f3f4c6804550"
      },
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the RAG Bot!\n",
            "Ask your question, or type 'exit' to quit.\n",
            "\n",
            "You: what are Apple products?\n",
            "Bot:\n",
            "Apple and the Apple logo are trademarks of Apple Inc. registered in the U.S.and other countries and regions.\n",
            "\n",
            "* Based on data reported to Apple by its suppliers.\n",
            "\n",
            "We’re also finding new and better ways to get Apple products into people’s hands.\n",
            "\n",
            "\n",
            "You: List the Aplle products\n",
            "Bot:\n",
            "Eligible products are those in a product category for which EPEAT registration exists, including workstations, desktops, laptops, displays, mobile phones, and tablets.\n",
            "\n",
            "52 Breakdown of U.S. retail packaging by weight.\n",
            "\n",
            "Our suppliers use this library to select the materials they use in our products.\n",
            "\n",
            "\n",
            "You: what is IOS?\n",
            "Bot:\n",
            "iOS is a trademark or registered trademark of Cisco in the U.S. and other countries and is used under license.\n",
            "\n",
            "The intended users of the assurance statement are the stakeholders of Apple.\n",
            "\n",
            "The intended users of the assurance statement are the stakeholders of Apple .\n",
            "\n",
            "\n",
            "You: Which Apple products are good for students?\n",
            "Bot:\n",
            "We’re also finding new and better ways to get Apple products into people’s hands.\n",
            "\n",
            "Eligible products are those in a product category for which EPEAT registration exists, including workstations, desktops, laptops, displays, mobile phones, and tablets.\n",
            "\n",
            "To support Malaika students and teachers, this solar initiative builds on other Apple collaborations, including virtual Today at Apple sessions on coding with Swift, photography, filmmaking, and design, as well as other events with Apple Retail teams and Diversity Network Associations.\n",
            "\n",
            "\n",
            "You: what is ipad?\n",
            "Bot:\n",
            "The initial pilot, launched in 2022, focused on reducing waste associated with iPad; the program now includes iPhone, Mac, Apple Watch, and AirPods.\n",
            "\n",
            "In 2022, we introduced these innovations for iPhone, iMac, iPad, and Apple Watch.\n",
            "\n",
            "By February 2024, m ore than 75 percent of all iPhone devices introduced in the last four years had updated to iOS 17, and iPadOS 17 was being used on more than 60 percent of iPad devices introduced in the last four years.\n",
            "\n",
            "\n",
            "You: Founder of Apple\n",
            "Bot:\n",
            "Interviews with relevant personnel of Apple; 2. Review of documentary evidence produced by Apple ; 3.\n",
            "\n",
            "Interviews with relevant personnel of Apple ; 2. Review of internal and external documentary evidence produced by Apple ; 3.\n",
            "\n",
            "Interviews with relevant personnel of Apple ; 3. Review of internal and external documentary evidence produced by Apple ; 4.\n",
            "\n",
            "\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9HHmrNAwrqA"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}